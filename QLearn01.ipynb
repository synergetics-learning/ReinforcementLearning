{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade2f5c5",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af55a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08188125",
   "metadata": {},
   "source": [
    "### Create an environment and get its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d095ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-14 14:20:47,407] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "n_observations = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3cad9",
   "metadata": {},
   "source": [
    "### Initialize the Q-table to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02929cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Q_table = np.zeros((n_observations,n_actions))\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9fa55",
   "metadata": {},
   "source": [
    "### Initialize Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74218f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of episode we will run\n",
    "n_episodes = 10000\n",
    "\n",
    "#maximum of iteration per episode\n",
    "max_iter_episode = 100\n",
    "\n",
    "#initialize the exploration probability to 1\n",
    "exploration_proba = 1\n",
    "\n",
    "#exploartion decreasing decay for exponential decreasing\n",
    "exploration_decreasing_decay = 0.001\n",
    "\n",
    "# minimum of exploration proba\n",
    "min_exploration_proba = 0.01\n",
    "\n",
    "#discounted factor\n",
    "gamma = 0.99\n",
    "\n",
    "#learning rate\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c74a6",
   "metadata": {},
   "source": [
    "### Declare a list to store rewards we get after each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a25fc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_per_episode = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b533eb0",
   "metadata": {},
   "source": [
    "### Main Loop to iterate over episodes to calculate rewards and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8367729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we iterate over episodes\n",
    "for e in range(n_episodes):\n",
    "    #we initialize the first state of the episode\n",
    "    current_state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #sum the rewards that the agent gets from the environment\n",
    "    total_episode_reward = 0\n",
    "    \n",
    "    for i in range(max_iter_episode): \n",
    "        # we sample a float from a uniform distribution over 0 and 1\n",
    "        # if the sampled flaot is less than the exploration proba\n",
    "        #     the agent selects arandom action\n",
    "        # else\n",
    "        #     he exploits his knowledge using the bellman equation \n",
    "        \n",
    "        if np.random.uniform(0,1) < exploration_proba:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state,:])\n",
    "        \n",
    "        # The environment runs the chosen action and returns\n",
    "        # the next state, a reward and true if the epiosed is ended.\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # We update our Q-table using the Q-learning iteration\n",
    "        Q_table[current_state, action] = (1-lr) * Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:]))\n",
    "        total_episode_reward = total_episode_reward + reward\n",
    "        # If the episode is finished, we leave the for loop\n",
    "        if done:\n",
    "            break\n",
    "        current_state = next_state\n",
    "    #We update the exploration proba using exponential decay formula \n",
    "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n",
    "    rewards_per_episode.append(total_episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627124d",
   "metadata": {},
   "source": [
    "### Print all rewards and observe, with each iteration, the performance is increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ff42104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward per thousand episodes\n",
      "1000 : mean espiode reward:  0.047\n",
      "2000 : mean espiode reward:  0.205\n",
      "3000 : mean espiode reward:  0.445\n",
      "4000 : mean espiode reward:  0.559\n",
      "5000 : mean espiode reward:  0.665\n",
      "6000 : mean espiode reward:  0.71\n",
      "7000 : mean espiode reward:  0.685\n",
      "8000 : mean espiode reward:  0.71\n",
      "9000 : mean espiode reward:  0.668\n",
      "10000 : mean espiode reward:  0.676\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean reward per thousand episodes\")\n",
    "for i in range(10):\n",
    "    print((i+1)*1000,\": mean espiode reward: \", np.mean(rewards_per_episode[1000*i:1000*(i+1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e202cc",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ffebe",
   "metadata": {},
   "source": [
    "1. The performance is bad at the beginning but keeps improving with each iteration.\n",
    "2. Q-learning algorithm is a very efficient way for an agent to learn how the environment works.\n",
    "3. The agent would also need many more episodes to learn about the environment and deliver as per the expected performance.\n",
    "4. As a solution, we can use a Deep Neural Network (DNN) to approximate the Q-Value function since DNNs are known for their efficiency to approximate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248dc766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
